{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This holds the main flow of the application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prestonrank/RAGMessages\n"
     ]
    }
   ],
   "source": [
    "# includes all main imports from sub-directories\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import sqlite3\n",
    "from pprint import pprint # text formatting \n",
    "from dotenv import load_dotenv  # from python-dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "\n",
    "project_dir = os.getcwd() \n",
    "print(project_dir)\n",
    "\n",
    "message_dir = \"data/processing\"\n",
    "sys.path.append(message_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen embedding approach : Asymmetric Semantic Search \n",
    "\n",
    "For the first MVP I will be using this instead of symmetric search because the queries themselves are not symmetrical. For an example query like \"SUBJECT_NAME : When was I really annoyed at this person?\" the query maps to a longer block of text containing the relevant information. \n",
    "\n",
    "This varies in comparison to a query like \"How to learn Javascript\" and finding an entry similar to \"How to learn JavaScript on the web?\", where this would be symmetrical. \n",
    "\n",
    "Pre-Trained MS MARCO Models will be used for this first implementation. \n",
    "Specifically, models tuned with normalized embeddings will be used instead of models tuned with dot products initially, normalized embeddings are more generalized, but dot products can be used as experimentation later, as they are more dynamic and may pick up additional semantic information.\n",
    "\n",
    "SentenceTransformer.encode_query and SentenceTransformer.encode_document specifically used for encoding the corpus as well as query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"multi-qa-mpnet-base-cos-v1\" # dim-size 384\n",
    "embedder = SentenceTransformer(bert_model_name, similarity_fn_name=SimilarityFunction.COSINE) # \n",
    "\n",
    "current_dim = 768\n",
    "\n",
    "input = [] \n",
    "input.append(\"This is a test input\") \n",
    "embeddings = embedder.encode(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of messages from data : 4992\n",
      "('2024-10-08 01:31:13', '', '', 'OOUU itâ€™s so fun seeing how other people edit stuff', '+19365539666')\n",
      "messages written to data txt : 4992\n",
      "[NULL] values added to text file 8\n",
      "number of text messages : 5000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import file\n",
    "\n",
    "subject_phone = \"9365539666\"\n",
    "subject_name = \"Paris\" \n",
    "messages_per_subject = 5000\n",
    "\n",
    "file.addToTextFile(subject_phone, messages_per_subject, subject_name) # puts data into text file \n",
    "\n",
    "sentences_per_embedding = 1 # sentences per embedding\n",
    "index_multiplier = 1 * sentences_per_embedding # looking for indexes in the text file \n",
    "\n",
    "batch_data = file.getTextFile(sentences_per_embedding)\n",
    "\n",
    "corpus = [] \n",
    "\n",
    "sentences = \"\"\n",
    "for batch in batch_data:\n",
    "  for sentence in batch: \n",
    "    sentences += sentence + \" \"\n",
    "  corpus.append(sentences)\n",
    "  sentences = \"\"\n",
    "print(\"number of text messages : \" + str(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = embedder.encode_document(corpus) \n",
    "\n",
    "embeddings = torch.tensor(np_embeddings, dtype=torch.float32)\n",
    "\n",
    "num_of_vectors = len(np_embeddings)\n",
    "print(embeddings.shape)\n",
    "print(\"Numbers of embedding vectors \" + str(num_of_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "\n",
    "index_one = faiss.IndexFlatL2(current_dim) # per subject, euclidean distance\n",
    "global_index = faiss.IndexFlatL2(current_dim) # includes all of the clusters \n",
    "\n",
    "index_one.add(embeddings) # to add an embedding, shape : (n_vectors, current_dim)\n",
    "# index.is_trained, for seeing if the index is trained \n",
    "print(\"Number of vectors in FAISS : \" + str((index_one.ntotal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_one = \"What time is the kickback?\"\n",
    "query_two = \"all kickbacks around fall\"\n",
    "\n",
    "current_query = query_two\n",
    "\n",
    "np_query_embedding = embedder.encode_query(current_query)\n",
    "\n",
    "query_embedding = torch.tensor(np_query_embedding, dtype=torch.float32)\n",
    "query_embedding = torch.unsqueeze(query_embedding, dim=0) # dim 1 to match the number of queries \n",
    "print(query_embedding.shape) \n",
    "\n",
    "print(\"Length of embedding \" + str(len(query_embedding)))\n",
    "\n",
    "k = 5\n",
    "xq = query_embedding # shape : (n_queries, dimension)\n",
    "# index.search finds the similar vectors in the FAISS DB \n",
    "D, I = index_one.search(xq, k) # I has shape : (number_of_queries, k), D has shape : (number_of_queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Indices matrix : \" + str(I.shape))\n",
    "print(\"Shape of Distances matrix : \" + str(D.shape))\n",
    "print(\"ID of indices : \" + str(I[0, :]) + \"\\n\")\n",
    "vec_ids = I[0, :].tolist()\n",
    "for index in vec_ids:\n",
    "    sentence = file.getTextFileLine(index, index_multiplier)\n",
    "    print(sentence)\n",
    "    vec = index_one.reconstruct(index)\n",
    "    reconstructed_vec = torch.from_numpy(vec)\n",
    "    query_tensor = torch.from_numpy(query_embedding.squeeze(0))  # Convert to tensor\n",
    "    similarity_score = embedder.similarity(query_tensor.unsqueeze(0), reconstructed_vec.unsqueeze(0))\n",
    "    euclidean_distance = torch.sqrt(torch.sum((query_tensor - reconstructed_vec) ** 2))\n",
    "    print(\"similarity score : \" + str(similarity_score))\n",
    "    print(\"euclidean distance score : \" + str(euclidean_distance) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text = \"They want to kickback on the Thursday after Halloweekend\"\n",
    "target_embedding = embedder.encode([target_text])\n",
    "query_embedding = embedder.encode([\"all kickbacks around fall\"])\n",
    "similarity = embedder.similarity(query_embedding, target_embedding)\n",
    "print(f\"Direct similarity: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
