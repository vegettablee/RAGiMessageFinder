{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This holds the main flow of the application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes all main imports from sub-directories\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import sqlite3\n",
    "from pprint import pprint # text formatting \n",
    "from dotenv import load_dotenv  # from python-dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "\n",
    "project_dir = os.getcwd() \n",
    "print(project_dir)\n",
    "\n",
    "message_dir = \"data/processing\"\n",
    "sys.path.append(message_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen embedding approach : Asymmetric Semantic Search \n",
    "\n",
    "For the first MVP I will be using this instead of symmetric search because the queries themselves are not symmetrical. For an example query like \"SUBJECT_NAME : When was I really annoyed at this person?\" the query maps to a longer block of text containing the relevant information. \n",
    "\n",
    "This varies in comparison to a query like \"How to learn Javascript\" and finding an entry similar to \"How to learn JavaScript on the web?\", where this would be symmetrical. \n",
    "\n",
    "Pre-Trained MS MARCO Models will be used for this first implementation. \n",
    "Specifically, models tuned with normalized embeddings will be used instead of models tuned with dot products initially, normalized embeddings are more generalized, but dot products can be used as experimentation later, as they are more dynamic and may pick up additional semantic information.\n",
    "\n",
    "SentenceTransformer.encode_query and SentenceTransformer.encode_document specifically used for encoding the corpus as well as query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"multi-qa-mpnet-base-cos-v1\" # dim-size 384\n",
    "embedder = SentenceTransformer(bert_model_name, similarity_fn_name=SimilarityFunction.COSINE) # \n",
    "\n",
    "current_dim = 768\n",
    "\n",
    "input = [] \n",
    "input.append(\"This is a test input\") \n",
    "embeddings = embedder.encode(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import file\n",
    "from datetime import datetime\n",
    "\n",
    "subject_phone = \"9365539666\"\n",
    "subject_name = \"Paris\" \n",
    "messages_per_subject = 4700\n",
    "\n",
    "messages = file.addToTextFile(subject_phone, messages_per_subject, subject_name) # puts data into text file \n",
    "\n",
    "time_differences = [] \n",
    "\n",
    "\n",
    "for idx, message in enumerate(messages): \n",
    "  if idx == len(messages) - 2: # reach second to last element \n",
    "    break\n",
    "  first = datetime.strptime(messages[idx][0], '%Y-%m-%d %H:%M:%S')\n",
    "  second = datetime.strptime(messages[idx + 1][0], '%Y-%m-%d %H:%M:%S')\n",
    "  time_diff_minutes = abs((first - second).total_seconds() / 60)\n",
    "  if time_diff_minutes > (60 * 60 * 24 * 14): # if time is greater than two weeks, don't dilute the average\n",
    "    pass \n",
    "  time_differences.append(time_diff_minutes)\n",
    "  print(time_differences)\n",
    "  \n",
    "time_tensor = torch.tensor(time_differences)\n",
    "mean = torch.mean(time_tensor) \n",
    "standard_deviation = torch.std(time_tensor) \n",
    "\n",
    "print(\"This is the average distance in minutes between two text messages : \" + str(mean))\n",
    "print(\"This is the standard deviation across all text messages : \" + str(standard_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Regular histogram\n",
    "axes[0, 0].hist(time_differences, bins=50, edgecolor='black', color='steelblue')\n",
    "axes[0, 0].set_xlabel('Time Difference (minutes)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Time Differences', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(mean.item(), color='red', linestyle='--', linewidth=2, label=f'Mean: {mean.item():.2f} min')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Logarithmic histogram (y-axis log scale)\n",
    "axes[0, 1].hist(time_differences, bins=50, edgecolor='black', color='coral')\n",
    "axes[0, 1].set_xlabel('Time Difference (minutes)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency (log scale)', fontsize=12)\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].set_title('Distribution with Log Scale', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(mean.item(), color='red', linestyle='--', linewidth=2, label=f'Mean: {mean.item():.2f} min')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Box plot to visualize outliers and quartiles\n",
    "axes[1, 0].boxplot(time_differences, vert=True, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "axes[1, 0].set_ylabel('Time Difference (minutes)', fontsize=12)\n",
    "axes[1, 0].set_title('Box Plot: Outliers & Spread', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Log-transformed histogram (x-axis log scale)\n",
    "# Filter out zeros to avoid log(0)\n",
    "time_diffs_nonzero = [t for t in time_differences if t > 0]\n",
    "axes[1, 1].hist(time_diffs_nonzero, bins=50, edgecolor='black', color='mediumpurple')\n",
    "axes[1, 1].set_xlabel('Time Difference (minutes, log scale)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].set_title('Distribution with Log-Transformed X-axis', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display percentiles\n",
    "percentiles = [25, 50, 75, 90, 95, 99]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean:              {mean.item():.2f} minutes ({mean.item()/60:.2f} hours)\")\n",
    "print(f\"Std Deviation:     {standard_deviation.item():.2f} minutes ({standard_deviation.item()/60:.2f} hours)\")\n",
    "print(f\"Std/Mean Ratio:    {standard_deviation.item()/mean.item():.2f}x\")\n",
    "print(f\"\\nTotal messages:    {len(time_differences)}\")\n",
    "print(f\"Min time diff:     {min(time_differences):.2f} minutes\")\n",
    "print(f\"Max time diff:     {max(time_differences):.2f} minutes ({max(time_differences)/60:.2f} hours)\")\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(time_differences, p)\n",
    "    print(f\"  {p}th percentile: {val:.2f} minutes ({val/60:.2f} hours)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import file\n",
    "\n",
    "subject_phone = \"9365539666\"\n",
    "subject_name = \"Paris\" \n",
    "messages_per_subject = 5000\n",
    "\n",
    "file.addToTextFile(subject_phone, messages_per_subject, subject_name) # puts data into text file \n",
    "\n",
    "sentences_per_embedding = 1 # sentences per embedding\n",
    "index_multiplier = 1 * sentences_per_embedding # looking for indexes in the text file \n",
    "\n",
    "batch_data = file.getTextFile(sentences_per_embedding)\n",
    "\n",
    "corpus = [] \n",
    "\n",
    "sentences = \"\"\n",
    "for batch in batch_data:\n",
    "  for sentence in batch: \n",
    "    sentences += sentence + \" \"\n",
    "  corpus.append(sentences)\n",
    "  sentences = \"\"\n",
    "print(\"number of text messages : \" + str(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = embedder.encode_document(corpus) \n",
    "\n",
    "embeddings = torch.tensor(np_embeddings, dtype=torch.float32)\n",
    "\n",
    "num_of_vectors = len(np_embeddings)\n",
    "print(embeddings.shape)\n",
    "print(\"Numbers of embedding vectors \" + str(num_of_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "\n",
    "index_one = faiss.IndexFlatL2(current_dim) # per subject, euclidean distance\n",
    "global_index = faiss.IndexFlatL2(current_dim) # includes all of the clusters \n",
    "\n",
    "index_one.add(embeddings) # to add an embedding, shape : (n_vectors, current_dim)\n",
    "# index.is_trained, for seeing if the index is trained \n",
    "print(\"Number of vectors in FAISS : \" + str((index_one.ntotal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_one = \"What time is the kickback?\"\n",
    "query_two = \"all kickbacks around fall\"\n",
    "\n",
    "current_query = query_two\n",
    "\n",
    "np_query_embedding = embedder.encode_query(current_query)\n",
    "\n",
    "query_embedding = torch.tensor(np_query_embedding, dtype=torch.float32)\n",
    "query_embedding = torch.unsqueeze(query_embedding, dim=0) # dim 1 to match the number of queries \n",
    "print(query_embedding.shape) \n",
    "\n",
    "print(\"Length of embedding \" + str(len(query_embedding)))\n",
    "\n",
    "k = 5\n",
    "xq = query_embedding # shape : (n_queries, dimension)\n",
    "# index.search finds the similar vectors in the FAISS DB \n",
    "D, I = index_one.search(xq, k) # I has shape : (number_of_queries, k), D has shape : (number_of_queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Indices matrix : \" + str(I.shape))\n",
    "print(\"Shape of Distances matrix : \" + str(D.shape))\n",
    "print(\"ID of indices : \" + str(I[0, :]) + \"\\n\")\n",
    "vec_ids = I[0, :].tolist()\n",
    "for index in vec_ids:\n",
    "    sentence = file.getTextFileLine(index, index_multiplier)\n",
    "    print(sentence)\n",
    "    vec = index_one.reconstruct(index)\n",
    "    reconstructed_vec = torch.from_numpy(vec)\n",
    "    query_tensor = torch.from_numpy(query_embedding.squeeze(0))  # Convert to tensor\n",
    "    similarity_score = embedder.similarity(query_tensor.unsqueeze(0), reconstructed_vec.unsqueeze(0))\n",
    "    euclidean_distance = torch.sqrt(torch.sum((query_tensor - reconstructed_vec) ** 2))\n",
    "    print(\"similarity score : \" + str(similarity_score))\n",
    "    print(\"euclidean distance score : \" + str(euclidean_distance) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text = \"They want to kickback on the Thursday after Halloweekend\"\n",
    "target_embedding = embedder.encode([target_text])\n",
    "query_embedding = embedder.encode([\"all kickbacks around fall\"])\n",
    "similarity = embedder.similarity(query_embedding, target_embedding)\n",
    "print(f\"Direct similarity: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
