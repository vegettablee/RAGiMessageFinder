{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This holds the main flow of the application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes all main imports from sub-directories\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import sqlite3\n",
    "from pprint import pprint # text formatting \n",
    "from dotenv import load_dotenv  # from python-dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"GPT_API_KEY\"))\n",
    "\n",
    "project_dir = os.getcwd() \n",
    "print(project_dir)\n",
    "\n",
    "message_dir = \"data/processing\"\n",
    "sys.path.append(message_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen embedding approach : Asymmetric Semantic Search \n",
    "\n",
    "For the first MVP I will be using this instead of symmetric search because the queries themselves are not symmetrical. For an example query like \"SUBJECT_NAME : When was I really annoyed at this person?\" the query maps to a longer block of text containing the relevant information. \n",
    "\n",
    "This varies in comparison to a query like \"How to learn Javascript\" and finding an entry similar to \"How to learn JavaScript on the web?\", where this would be symmetrical. \n",
    "\n",
    "Pre-Trained MS MARCO Models will be used for this first implementation. \n",
    "Specifically, models tuned with normalized embeddings will be used instead of models tuned with dot products initially, normalized embeddings are more generalized, but dot products can be used as experimentation later, as they are more dynamic and may pick up additional semantic information.\n",
    "\n",
    "SentenceTransformer.encode_query and SentenceTransformer.encode_document specifically used for encoding the corpus as well as query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_embedding_model = \"text-embedding-3-large\" # dim-size :  1536  \n",
    "small_embedding_model = \"text-embedding-3-small\" # dim-size : 3072 \n",
    "\n",
    "bert_model_name = \"msmarco-MiniLM-L6-cos-v5\" # dim-size 384\n",
    "embedder = SentenceTransformer(bert_model_name) \n",
    "\n",
    "current_dim = 384\n",
    "\n",
    "input = [] \n",
    "input.append(\"This is a test input\") \n",
    "embeddings = embedder.encode(input)\n",
    "print(embeddings) \n",
    "\n",
    "# response = client.embeddings.create(\n",
    "    # input=test_input,\n",
    "   #  model=current_model\n",
    "# )\n",
    "#print(response.data[0].embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd())\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data.txt\"\n",
    "print(f\"File name: {file_name}\")\n",
    "print(f\"Sentences per embedding: {sentences_per_embedding}\")\n",
    "\n",
    "# Also check the raw file content:\n",
    "with open(file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"Total lines in file: {len(lines)}\")\n",
    "    for i, line in enumerate(lines):\n",
    "        print(f\"Line {i}: '{line.strip()}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import file\n",
    "\n",
    "subject_phone = \"\"\n",
    "subject_name = \"\" \n",
    "messages_per_subject = 100\n",
    "\n",
    "file.addToTextFile(subject_phone, messages_per_subject) # puts data into text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_per_embedding = 2 # sentences per embedding\n",
    "index_multiplier = 1 * sentences_per_embedding # looking for indexes in the text file \n",
    "\n",
    "batch_data = file.getTextFile(sentences_per_embedding)\n",
    "\n",
    "corpus = [] \n",
    "\n",
    "for batch in batch_data:\n",
    "  for sentence in batch: \n",
    "    sentences += sentence + \" \"\n",
    "  corpus.append(sentences)\n",
    "  print(sentences) \n",
    "  sentences = \"\"\n",
    "\n",
    "# conn.close() closes the connection to the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = embedder.encode_document(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.tensor(np_embeddings, dtype=torch.float32)\n",
    "\n",
    "num_of_vectors = len(np_embeddings)\n",
    "print(embeddings.shape)\n",
    "print(\"Numbers of embedding vectors \" + str(num_of_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Did someone's family ever get killed?\"\n",
    "\n",
    "np_query_embedding = embedder.encode_query(query)\n",
    "\n",
    "query_embedding = torch.tensor(np_query_embedding, dtype=torch.float32)\n",
    "query_embedding = torch.unsqueeze(query_embedding, dim=0) # dim 1 to match the number of queries \n",
    "print(query_embedding.shape) \n",
    "\n",
    "print(\"Length of embedding \" + str(len(query_embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "\n",
    "index_one = faiss.IndexFlatL2(current_dim) # per subject \n",
    "global_index = faiss.IndexFlatL2(current_dim) # includes all of the clusters \n",
    "\n",
    "index_one.add(embeddings) # to add an embedding, shape : (n_vectors, current_dim)\n",
    "# index.is_trained, for seeing if the index is trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of vectors in FAISS : \" + str((index_one.ntotal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "xq = query_embedding # shape : (n_queries, dimension)\n",
    "# index.search finds the similar vectors in the FAISS DB \n",
    "D, I = index_one.search(xq, k) # I has shape : (number_of_queries, k), D has shape : (number_of_queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Indices matrix : \" + str(I.shape))\n",
    "print(\"Shape of Distances matrix : \" + str(D.shape))\n",
    "print(\"ID of indices : \" + str(I[0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_ids = I[0, :].tolist()# this the first index returned by FAISS, int is because it originally returns as a numpy int\n",
    "for index in vec_ids: \n",
    "  sentence = file.getTextFileLine(index, index_multiplier)\n",
    "  print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
