{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This holds the main flow of the application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/Users/prestonrank/RAGMessages\n"
     ]
    }
   ],
   "source": [
    "# includes all main imports from sub-directories\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import sqlite3\n",
    "from pprint import pprint # text formatting \n",
    "from dotenv import load_dotenv  # from python-dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"GPT_API_KEY\"))\n",
    "\n",
    "project_dir = os.getcwd() \n",
    "print(project_dir)\n",
    "\n",
    "message_dir = \"data/processing\"\n",
    "sys.path.append(message_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen embedding approach : Asymmetric Semantic Search \n",
    "\n",
    "For the first MVP I will be using this instead of symmetric search because the queries themselves are not symmetrical. For an example query like \"SUBJECT_NAME : When was I really annoyed at this person?\" the query maps to a longer block of text containing the relevant information. \n",
    "\n",
    "This varies in comparison to a query like \"How to learn Javascript\" and finding an entry similar to \"How to learn JavaScript on the web?\", where this would be symmetrical. \n",
    "\n",
    "Pre-Trained MS MARCO Models will be used for this first implementation. \n",
    "Specifically, models tuned with normalized embeddings will be used instead of models tuned with dot products initially, normalized embeddings are more generalized, but dot products can be used as experimentation later, as they are more dynamic and may pick up additional semantic information.\n",
    "\n",
    "SentenceTransformer.encode_query and SentenceTransformer.encode_document specifically used for encoding the corpus as well as query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_embedding_model = \"text-embedding-3-large\" # dim-size :  1536  \n",
    "small_embedding_model = \"text-embedding-3-small\" # dim-size : 3072 \n",
    "\n",
    "bert_model_name = \"multi-qa-MiniLM-L6-cos-v1\" # dim-size 384\n",
    "embedder = SentenceTransformer(bert_model_name, similarity_fn_name=SimilarityFunction.COSINE) # \n",
    "\n",
    "current_dim = 384\n",
    "\n",
    "input = [] \n",
    "input.append(\"This is a test input\") \n",
    "embeddings = embedder.encode(input)\n",
    "# print(embeddings) \n",
    "\n",
    "# response = client.embeddings.create(\n",
    "    # input=test_input,\n",
    "   #  model=current_model\n",
    "# )\n",
    "#print(response.data[0].embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of messages from data : 4992\n",
      "messages written to data txt : 4992\n",
      "[NULL] values added to text file 8\n",
      "number of text messages : 834\n"
     ]
    }
   ],
   "source": [
    "import file\n",
    "\n",
    "subject_phone = \"9365539666\"\n",
    "subject_name = \"Paris\" \n",
    "messages_per_subject = 5000\n",
    "\n",
    "file.addToTextFile(subject_phone, messages_per_subject) # puts data into text file \n",
    "\n",
    "sentences_per_embedding = 6 # sentences per embedding\n",
    "index_multiplier = 1 * sentences_per_embedding # looking for indexes in the text file \n",
    "\n",
    "batch_data = file.getTextFile(sentences_per_embedding)\n",
    "\n",
    "corpus = [] \n",
    "\n",
    "sentences = \"\"\n",
    "for batch in batch_data:\n",
    "  for sentence in batch: \n",
    "    sentences += sentence + \" \"\n",
    "  corpus.append(sentences)\n",
    "  sentences = \"\"\n",
    "print(\"number of text messages : \" + str(len(corpus)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([834, 384])\n",
      "Numbers of embedding vectors 834\n"
     ]
    }
   ],
   "source": [
    "np_embeddings = embedder.encode_document(corpus) \n",
    "\n",
    "embeddings = torch.tensor(np_embeddings, dtype=torch.float32)\n",
    "\n",
    "num_of_vectors = len(np_embeddings)\n",
    "print(embeddings.shape)\n",
    "print(\"Numbers of embedding vectors \" + str(num_of_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in FAISS : 834\n"
     ]
    }
   ],
   "source": [
    "import faiss \n",
    "\n",
    "index_one = faiss.IndexFlatL2(current_dim) # per subject, euclidean distance\n",
    "global_index = faiss.IndexFlatL2(current_dim) # includes all of the clusters \n",
    "\n",
    "index_one.add(embeddings) # to add an embedding, shape : (n_vectors, current_dim)\n",
    "# index.is_trained, for seeing if the index is trained \n",
    "print(\"Number of vectors in FAISS : \" + str((index_one.ntotal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n",
      "Length of embedding 1\n"
     ]
    }
   ],
   "source": [
    "query_one = \"What time is the kickback?\"\n",
    "query_two = \"all kickbacks around fall\"\n",
    "\n",
    "current_query = query_two\n",
    "\n",
    "np_query_embedding = embedder.encode_query(current_query)\n",
    "\n",
    "query_embedding = torch.tensor(np_query_embedding, dtype=torch.float32)\n",
    "query_embedding = torch.unsqueeze(query_embedding, dim=0) # dim 1 to match the number of queries \n",
    "print(query_embedding.shape) \n",
    "\n",
    "print(\"Length of embedding \" + str(len(query_embedding)))\n",
    "\n",
    "k = 5\n",
    "xq = query_embedding # shape : (n_queries, dimension)\n",
    "# index.search finds the similar vectors in the FAISS DB \n",
    "D, I = index_one.search(xq, k) # I has shape : (number_of_queries, k), D has shape : (number_of_queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Indices matrix : (1, 5)\n",
      "Shape of Distances matrix : (1, 5)\n",
      "ID of indices : [102 817 257 818 166]\n",
      "\n",
      "['[THEM] ALSO\\n', '[THEM] They want to kickback on the Thursday after Halloweekend\\n', '[THEM] Cause some people are leaving friday\\n', '[THEM] Couldn‚Äôt tell bro, looked fried\\n', '[ME] yeah the kickback friday is chill too\\n', '[ME] i‚Äôm fried\\n']\n",
      "similarity score : tensor([[0.3572]])\n",
      "euclidean distance score : tensor(1.1339)\n",
      "\n",
      "['[ME] we were high like majority of the time üò≠\\n', '[ME] and drove back at 2am and got back in plano at 6am\\n', '[ME] insane experience\\n', '[THEM] DAMN early af for why? üò≠\\n', '[THEM] That‚Äôs so fun tho üôÇ\\u200d‚ÜïÔ∏è\\n', '[THEM] Alternatively Paola and I go and come back and then you join for mini kickback\\n']\n",
      "similarity score : tensor([[0.3245]])\n",
      "euclidean distance score : tensor(1.1623)\n",
      "\n",
      "['[THEM] Dude. I just got sprayed by a sprinkler again\\n', '[THEM] Wth, it literally popped out of the ground and IMMEDIATELY hit meüò≠\\n', '[ME] LMFAOOO\\n', '[ME] you should‚Äôve learned your lesson üò≠\\n', '[ME] never go down that path again\\n', '[THEM] I WENT DOWN A DIFFERENT WAY\\n']\n",
      "similarity score : tensor([[0.2908]])\n",
      "euclidean distance score : tensor(1.1909)\n",
      "\n",
      "['[ME] i‚Äôll join y‚Äôall for the kickback, i gotta decompress after work üò≠\\n', '[THEM] Scratch that, just kickback üò≠\\n', '[THEM] Want any snacks? We could do little dinner, puzzle, drinks, and watch smth\\n', '[ME] okay bet\\n', '[ME] what time tho\\n', '[ME] i just got off of work\\n']\n",
      "similarity score : tensor([[0.2892]])\n",
      "euclidean distance score : tensor(1.1923)\n",
      "\n",
      "['[THEM] I def was hitting speeds I don‚Äôt usually hit üò≠üò≠\\n', '[ME] man you been tweaking a lot üò≠\\n', '[THEM] üò≠üò≠\\n', '[THEM] üôè\\n', '[THEM] It‚Äôs just tweak season, it‚Äôll pass üëπ\\n', '[ME] ü§•\\n']\n",
      "similarity score : tensor([[0.2732]])\n",
      "euclidean distance score : tensor(1.2057)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Indices matrix : \" + str(I.shape))\n",
    "print(\"Shape of Distances matrix : \" + str(D.shape))\n",
    "print(\"ID of indices : \" + str(I[0, :]) + \"\\n\")\n",
    "vec_ids = I[0, :].tolist()\n",
    "for index in vec_ids:\n",
    "    sentence = file.getTextFileLine(index, index_multiplier)\n",
    "    print(sentence)\n",
    "    vec = index_one.reconstruct(index)\n",
    "    reconstructed_vec = torch.from_numpy(vec)\n",
    "    query_tensor = torch.from_numpy(query_embedding.squeeze(0))  # Convert to tensor\n",
    "    similarity_score = embedder.similarity(query_tensor.unsqueeze(0), reconstructed_vec.unsqueeze(0))\n",
    "    euclidean_distance = torch.sqrt(torch.sum((query_tensor - reconstructed_vec) ** 2))\n",
    "    print(\"similarity score : \" + str(similarity_score))\n",
    "    print(\"euclidean distance score : \" + str(euclidean_distance) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct similarity: tensor([[0.3974]])\n"
     ]
    }
   ],
   "source": [
    "target_text = \"They want to kickback on the Thursday after Halloweekend\"\n",
    "target_embedding = embedder.encode([target_text])\n",
    "query_embedding = embedder.encode([\"all kickbacks around fall\"])\n",
    "similarity = embedder.similarity(query_embedding, target_embedding)\n",
    "print(f\"Direct similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
